{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer,TFAutoModel,pipeline,TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokens : ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\ninput ids : [7993, 170, 13809, 23763, 2443, 1110, 3014]\nfinal input ids :[101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102]\n"
     ]
    }
   ],
   "source": [
    "tokens=tokenizer.tokenize('Using a Transformer network is simple')\n",
    "print(f'tokens : {tokens}')\n",
    "input_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f'input ids : {input_ids}')\n",
    "final_inputs=tokenizer.prepare_for_model(input_ids)\n",
    "print(f\"final input ids :{final_inputs['input_ids']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show how the tokenizer transformed sentence into tokens\n",
    "decoded_string=tokenizer.decode(final_inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] Using a Transformer network is simple [SEP]'"
      ]
     },
     "metadata": {},
     "execution_count": 180
    }
   ],
   "source": [
    "decoded_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Experiment with Auto Model and custom toknization, attention, tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_138']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "distbert_tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "distbert_model=TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=[\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=distbert_tokenizer(sents,padding=True,truncation=True,return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
       "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
       "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
       "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])>}"
      ]
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_logits=distbert_model(tokens).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-1.5606962,  1.6122811],\n",
       "       [ 4.1692314, -3.3464477]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "source": [
    "outputs_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "tf.argmax(tf.math.softmax(outputs_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "distbert_model.config.id2label\n"
   ]
  },
  {
   "source": [
    "### Using Tokenization, attention mask,"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2=AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_158']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2=TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I've been waiting for a HuggingFace course my whole life.\nI hate this so much!\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[tokenizer2.tokenize(sent) for sent in sents]\n",
    "input_ids=[tokenizer.convert_tokens_to_ids(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.'], ['i', 'hate', 'this', 'so', 'much', '!']] [[178, 112, 1396, 1151, 2613, 1111, 170, 19558, 10931, 1736, 1139, 2006, 1297, 119], [178, 4819, 1142, 1177, 1277, 106]]\n"
     ]
    }
   ],
   "source": [
    "print(tokens,input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14 [178, 112, 1396, 1151, 2613, 1111, 170, 19558, 10931, 1736, 1139, 2006, 1297, 119]\n6 [178, 4819, 1142, 1177, 1277, 106]\n"
     ]
    }
   ],
   "source": [
    "for input_id in input_ids:\n",
    "    print(len(input_id),input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[1]=input_ids[1]+[0,0,0,0,0,0,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14 [178, 112, 1396, 1151, 2613, 1111, 170, 19558, 10931, 1736, 1139, 2006, 1297, 119]\n14 [178, 4819, 1142, 1177, 1277, 106, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for input_id in input_ids:\n",
    "    print(len(input_id),input_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=tf.constant(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 14), dtype=int32, numpy=\n",
       "array([[  178,   112,  1396,  1151,  2613,  1111,   170, 19558, 10931,\n",
       "         1736,  1139,  2006,  1297,   119],\n",
       "       [  178,  4819,  1142,  1177,  1277,   106,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]])>"
      ]
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits=model2(input_ids).logits ## see output logits and this output logits are different, this is since model giving attentions 0s as separate features. ideally it should be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 1.6735691 , -1.4508451 ],\n",
       "       [ 1.0350972 , -0.92355907]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets give attension_mask\n",
    "attention_mask1=tf.constant(tf.ones((1,14),dtype=tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask2=tf.concat([tf.ones((1,7),dtype=tf.int64),tf.zeros((1,7),dtype=tf.int64)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask=tf.concat([attention_mask1,attention_mask2],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 14), dtype=int64, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 205
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits=model2(input_ids,attention_mask=attention_mask).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 1.6735691, -1.4508451],\n",
       "       [ 1.3126377, -1.1695657]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=tf.math.softmax(output_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.9578886 , 0.04211135],\n",
       "       [0.92288476, 0.07711524]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1], dtype=int64)>"
      ]
     },
     "metadata": {},
     "execution_count": 210
    }
   ],
   "source": [
    "tf.argmax(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "metadata": {},
     "execution_count": 211
    }
   ],
   "source": [
    "model2.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}