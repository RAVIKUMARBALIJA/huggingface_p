{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "classifier=pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9479376077651978}]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "classifier(['I have been waiting for a long time for hugging face course'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9985089302062988}]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "classifier(['I have been waiting for a long time for hugging face course but i was disappointed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9989789128303528},\n",
       " {'label': 'POSITIVE', 'score': 0.9998844265937805}]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "classifier(['I dont have a new laptop with me','my wife is beautiful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 688/688 [00:00<00:00, 689kB/s]\n",
      "Downloading: 100%|██████████| 1.43G/1.43G [03:57<00:00, 6.00MB/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.31MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 813kB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.59MB/s]\n"
     ]
    }
   ],
   "source": [
    "zero_shot=pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sequence': 'this course is about transformers library',\n",
       " 'labels': ['education', 'business', 'politics', 'sports'],\n",
       " 'scores': [0.7280060052871704,\n",
       "  0.10898680239915848,\n",
       "  0.08890784531831741,\n",
       "  0.0740993395447731]}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "zero_shot(['this course is about transformers library'],candidate_labels=['education','politics','business','sports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "text_gen=pipeline('text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': 'This course will teach you how to navigate through and execute web pages using Node.JS. We will share techniques to improve your understanding of the web and how to navigate through data over web pages.'}]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "text_gen(['This course will teach you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'generated_text': 'captain marvel is a super hero that inspires awe and admiration from those around them. The characters, actors, and directors make up their own team within the team, with various acting colleagues with specialties, and are known for their ability to create amazing'}],\n",
       " [{'generated_text': \"this hugging face course tells you that while it's a bit different for you then it shouldn't be that difficult to follow), then you are actually much better off with just your face doing it.\\n\\nStep two: If you are doing the hug\"}]]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "text_gen(['captain marvel is a super hero',\"this hugging face course tells you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': \"tell me about huggingface transformers and I'll give you my opinion.\\n\\n[youtube:https://youtu.be/Yp8LnEyE1tS4]\\n\\nThe fact that you cannot truly hug while being\"}]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "text_gen(['tell me about huggingface transformers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': 'huggingface transformers are experimented and tested in the studio. One of the only real-time results of the experiment is that for several seconds, the color appears green before it shifts to orange with each press.\\n\\nThe colors of the'}]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "text_gen(['huggingface transformers are experimented'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': 'huggingface transformers are experimented at the local level by all but the most well-known of the major European universities and institutions. They have developed the concept so that they can produce light-speed machines that make them invisible, and are therefore'},\n",
       " {'generated_text': 'huggingface transformers are experimented on to make them work by the user. They are supposed to create a high resolution glow around the user that appears like a bright orange glow, or green green glow as in the show. And because they use'},\n",
       " {'generated_text': 'huggingface transformers are experimented with in the past, and some were found to cause some damage on the ground.\\n\\nThe \"Catch\" button makes one\\'s character attack, and is the third power-up obtained through special moves'},\n",
       " {'generated_text': 'huggingface transformers are experimented on. These were supposed to do the same thing for a few months but have since been pushed back further to a year.\\n\\nThe best thing about the new devices is that they use a modified version of'},\n",
       " {'generated_text': \"huggingface transformers are experimented with with various types of devices and algorithms to test the effect.\\n\\nAdvertisement\\n\\nAdvertisement\\n\\nAdvertisement\\n\\nI was given a few days to try and catch up on all the games I'd just\"}]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "text_gen(['huggingface transformers are experimented'],num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "text_gen_cust=pipeline('text-generation',model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': \"Ravi kumar Balija is a nice guy and a nice guy. This book, if you read this and get it, isn't just about this man (Kumar) who has become a\"},\n",
       " {'generated_text': 'Ravi kumar Balija is a nice guy. I could easily do this for the rest of my life without any further embarrassment. My father is married to the same guy. And one of them'},\n",
       " {'generated_text': \"Ravi kumar Balija is a nice guy to hang out with. He's my teammate during the finals, and he's a good friend of mine.\\n\\nWhat's a team like you\"},\n",
       " {'generated_text': \"Ravi kumar Balija is a nice guy that was here for me a few months ago so I am happy about his return. I'm still waiting my turn.\\n\\nSajjan:\"}]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "text_gen_cust('Ravi kumar Balija is a nice guy',max_length=40,num_return_sequences=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 747/747 [00:00<00:00, 379kB/s]\n",
      "Downloading: 100%|██████████| 498M/498M [01:25<00:00, 5.80MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at lucone83/deep-metal.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.48MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 1.04MB/s]\n",
      "Downloading: 100%|██████████| 26.0/26.0 [00:00<00:00, 8.69kB/s]\n",
      "Downloading: 100%|██████████| 181/181 [00:00<00:00, 45.4kB/s]\n",
      "Downloading: 100%|██████████| 111/111 [00:00<00:00, 55.7kB/s]\n"
     ]
    }
   ],
   "source": [
    "text_gen_telugu=pipeline('text-generation',model='lucone83/deep-metal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': \"every night in my dreams\\nThey're gonna come\\nI heard it all\\n\\nThis is your fate\\nThis is your fate\\nYou're gonna die\\nI know it's real\\nI can't resist\\n\\nI'm looking at the stars\\nI wanna tell you a secret\\nNow it turns to ice\\nI'm looking at the stars\\nLooking for a place to stay\\n\\nMy blood runs cold\\nLike a river of blood\\nYou need the love of man\\nThe love of all who stand in your way\\nYour dreams are just a show\\nA dream that doesn't die\\n\\nThis is your fate\\nThis is your fate\\nYou're gonna die\\nI know it's real\\n\\nMy blood runs cold\\nLike a river of blood\\nYou need the love of man\\nThe love of all who stand in your way\\nYour dreams are just a show\\nA dream that doesn't die\\n\\nYour screams are like knives\\nA curse to the skies\\nYou're just a prisoner\\nAnd the shadows are my shield\\n\\nMy blood runs cold\\nLike a river of blood\\nYou need the love of man\\nThe love of all who stand in your way\\nYour dreams are just a show\\nA dream that doesn't die\\n\\n\"}]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "text_gen_telugu('every night in my dreams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': \"You think your life’s so grad-y?\\nYou think that you never will live on or go away\\nWell, this is gonna take it away, take it away\\nIt's gonna take it away...\\nOh, yeah, it's gonna take it away\\n\\nLivin' on the streets\\nAin't got something to eat\\nYour mom always had some\\nYou don't know you really feel it\\nAin't got you feeling the same\\nThere's not a day that ain'thin' like it\\n\\nNow, you know you've gotta know\\nYou gotta know it's gotta leave you\\nThere a million ways you can tell me\\nI'm down on my luck\\nBut there ain't nothing I can do\\nTo prove you know it\\nI ain't never wanna say\\nI ain't never gonna say\\n\\nLivin' on the streets\\nAin't got something to eat\\nYour mom always had some\\nYou don't know you really feel it\\nAin't got you feeling the same\\nThere's not a day that ain'thin' like it\\n\\nThere's not a day that ain'thin' like it \"}]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "text_gen_telugu('You think your life’s so grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 480/480 [00:00<00:00, 482kB/s]\n",
      "Downloading: 100%|██████████| 487M/487M [01:21<00:00, 5.96MB/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at distilroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.41MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 806kB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:01<00:00, 882kB/s]\n"
     ]
    }
   ],
   "source": [
    "mask_fill=pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'sequence': 'this course will teach you all about mathematical models',\n",
       "  'score': 0.1756952852010727,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'this course will teach you all about building models',\n",
       "  'score': 0.0412922129034996,\n",
       "  'token': 745,\n",
       "  'token_str': ' building'},\n",
       " {'sequence': 'this course will teach you all about predictive models',\n",
       "  'score': 0.03559696301817894,\n",
       "  'token': 27930,\n",
       "  'token_str': ' predictive'},\n",
       " {'sequence': 'this course will teach you all about role models',\n",
       "  'score': 0.03061850368976593,\n",
       "  'token': 774,\n",
       "  'token_str': ' role'},\n",
       " {'sequence': 'this course will teach you all about computer models',\n",
       "  'score': 0.02271265536546707,\n",
       "  'token': 3034,\n",
       "  'token_str': ' computer'}]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "mask_fill('this course will teach you all about <mask> models',top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 334kB/s]\n",
      "Downloading: 100%|██████████| 1.33G/1.33G [03:51<00:00, 5.76MB/s]\n",
      "Some layers from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing TFBertForTokenClassification: ['dropout_147']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 566kB/s]\n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 15.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "named_entity_rec=pipeline('ner',grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:161: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.identity instead.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9873192459344864,\n",
       "  'word': 'Ravi kumar',\n",
       "  'start': 5,\n",
       "  'end': 15},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.7057940363883972,\n",
       "  'word': '##sy',\n",
       "  'start': 35,\n",
       "  'end': 37},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.8828704357147217,\n",
       "  'word': 'hyderabad',\n",
       "  'start': 47,\n",
       "  'end': 56}]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "named_entity_rec('I am Ravi kumar and i work for infosys limted, hyderabad for the last 7 years. i have been to melbourne once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 473/473 [00:00<00:00, 236kB/s]\n",
      "Downloading: 100%|██████████| 261M/261M [00:42<00:00, 6.13MB/s]\n",
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_149']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 561kB/s]\n",
      "Downloading: 100%|██████████| 436k/436k [00:00<00:00, 1.04MB/s]\n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 9.70kB/s]\n"
     ]
    }
   ],
   "source": [
    "qun_answering=pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.9439806938171387, 'start': 11, 'end': 18, 'answer': 'infosys'}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "qun_answering(question='Where do i work',context='I work for infosys limited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.3977552354335785,\n",
       " 'start': 11,\n",
       " 'end': 35,\n",
       " 'answer': 'wipro limited, hyderabad'}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "qun_answering(question='Where do i work',context='I work for wipro limited, hyderabad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.5041349530220032, 'start': 28, 'end': 37, 'answer': 'hyderabad'}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "qun_answering(question='Where do i work',context='I work for wipro limited in hyderabad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'score': 0.5041349530220032, 'start': 28, 'end': 37, 'answer': 'hyderabad'}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "qun_answering(question='Where do i work',context='I work for wipro limited in hyderabad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 1.20k/1.20k [00:00<00:00, 300kB/s]\n",
      "Downloading: 100%|██████████| 242M/242M [00:39<00:00, 6.09MB/s]\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Downloading: 100%|██████████| 792k/792k [00:00<00:00, 1.33MB/s]\n",
      "Downloading: 100%|██████████| 1.39M/1.39M [00:00<00:00, 1.74MB/s]\n"
     ]
    }
   ],
   "source": [
    "summerizer=pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'summary_text': 'the number of graduates in traditional engineering disciplines has declined . in most of the premier american universities engineering curricula now concentrate on and encourage largely the study of engineering science . rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "summerizer(\"\"\"\n",
    "America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/tf_model.h5\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "Can't load weights for 'Helsinki-NLP/opus-mt-fr-en'. Make sure that:\n\n- 'Helsinki-NLP/opus-mt-fr-en' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'Helsinki-NLP/opus-mt-fr-en' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1239\u001b[0m                 \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1240\u001b[1;33m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[0;32m   1241\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1270\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1272\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1441\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m             \u001b[0metag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X-Linked-Etag\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ETag\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/Helsinki-NLP/opus-mt-fr-en/resolve/main/tf_model.h5",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-23df61793127>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"translation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Helsinki-NLP/opus-mt-fr-en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;31m# Infer the framework form the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mframework\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mframework\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfer_framework_from_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[0mtask_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"impl\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36minfer_framework_from_model\u001b[1;34m(model, model_classes, task, **model_kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_classes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTFAutoModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         raise ValueError(\n\u001b[0;32m    383\u001b[0m             \u001b[1;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1255\u001b[0m                     \u001b[1;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a file named one of {TF2_WEIGHTS_NAME}, {WEIGHTS_NAME}.\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m                 )\n\u001b[1;32m-> 1257\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1258\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresolved_archive_file\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0marchive_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loading weights file {archive_file}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load weights for 'Helsinki-NLP/opus-mt-fr-en'. Make sure that:\n\n- 'Helsinki-NLP/opus-mt-fr-en' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'Helsinki-NLP/opus-mt-fr-en' is the correct path to a directory containing a file named one of tf_model.h5, pytorch_model.bin.\n\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")"
   ]
  },
  {
   "source": [
    "##Limitations of Transformers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 286kB/s]\n",
      "Downloading: 100%|██████████| 536M/536M [01:37<00:00, 5.49MB/s]\n",
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 611kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.17MB/s]\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 28.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "fill_maks2=pipeline('fill-mask',model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=fill_maks2('This man works as a [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'sequence': 'this man works as a carpenter.',\n",
       "  'score': 0.0751064121723175,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter'},\n",
       " {'sequence': 'this man works as a lawyer.',\n",
       "  'score': 0.046419162303209305,\n",
       "  'token': 5160,\n",
       "  'token_str': 'lawyer'},\n",
       " {'sequence': 'this man works as a farmer.',\n",
       "  'score': 0.039145633578300476,\n",
       "  'token': 7500,\n",
       "  'token_str': 'farmer'},\n",
       " {'sequence': 'this man works as a businessman.',\n",
       "  'score': 0.03280142322182655,\n",
       "  'token': 6883,\n",
       "  'token_str': 'businessman'},\n",
       " {'sequence': 'this man works as a doctor.',\n",
       "  'score': 0.029292341321706772,\n",
       "  'token': 3460,\n",
       "  'token_str': 'doctor'}]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "[res['token_str'] for res in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "result=fill_maks2('This woman works as a [MASK].')\n",
    "[res['token_str'] for res in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}